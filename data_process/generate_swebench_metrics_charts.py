#!/usr/bin/env python3
"""
Generate Chart.js configurations for SWE-bench detailed metrics.

This script reads the swebench_detailed_metrics.json file (generated by extract_swebench_metrics.py)
and creates Chart.js configurations for each metric, with all models compared.

Each metric is represented as a separate Chart.js configuration in the output JSON file.

Usage:
    python generate_swebench_metrics_charts.py
    python generate_swebench_metrics_charts.py --input swebench_detailed_metrics.json --output swebench_metrics_charts.json --pretty

Output format:
{
    "metric_name1": { ... Chart.js configuration ... },
    "metric_name2": { ... Chart.js configuration ... },
    ...
}
"""

import json
import os
import argparse
import sys
from typing import Dict, List, Any, Tuple, Optional

# Import the chartjs_wrap module
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from chartjs_wrap import ChartJS, ChartType, Font, Position, TextAlign, Easing, ScaleType
from chartjs_wrap import BarDataset, LineDataset, RadarDataset, PieDataset, DoughnutDataset
from chartjs_wrap import TitleConfig, LegendConfig, TooltipConfig, ScaleConfig, ScaleTitle, GridLine
from chartjs_wrap import AnimationConfig, RadialLinearScale, AngleLine, PointLabel


def get_color_palette() -> List[Tuple[str, str]]:
    """Return a list of (border_color, background_color) pairs for models.

    Border color format: #RRGGBBFF (full opacity)
    Background color format: #RRGGBB22 or #RRGGBB33 (low opacity)
    """
    # Predefined color palette with distinct colors
    base_colors = [
        "#FF6384",  # Red
        "#36A2EB",  # Blue
        "#FFCE56",  # Yellow
        "#4BC0C0",  # Teal
        "#9966FF",  # Purple
        "#FF9F40",  # Orange
        "#C9CBCF",  # Gray
        "#7EB26D",  # Green
        "#E377C2",  # Pink
        "#1F77B4",  # Dark Blue
        "#FF7F0E",  # Dark Orange
        "#2CA02C",  # Dark Green
    ]

    palette = []
    for base in base_colors:
        # Remove # if present
        if base.startswith("#"):
            hex_code = base[1:]
        else:
            hex_code = base

        # Border color with full opacity (FF)
        border_color = f"#{hex_code}FF"

        # Background color with low opacity (22 or 33)
        # Alternate between 22 and 33 for variety
        if len(palette) % 2 == 0:
            background_color = f"#{hex_code}22"
        else:
            background_color = f"#{hex_code}33"

        palette.append((border_color, background_color))

    return palette


def load_metrics(input_file: str) -> Dict[str, Dict[str, Any]]:
    """Load metrics from JSON file.

    Args:
        input_file: Path to swebench_detailed_metrics.json

    Returns:
        Dictionary mapping model_name -> metrics_dict
    """
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data


def extract_all_metrics(models_data: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """Extract all metrics from models data.

    Args:
        models_data: Dictionary mapping model_name -> metrics_dict

    Returns:
        Dictionary mapping metric_name -> {model_name -> value}
    """
    metric_values = {}

    # First, collect all possible metric names
    all_metric_names = set()

    for model_name, metrics in models_data.items():
        # Add top-level metrics
        for key in metrics.keys():
            if key != 'test_stats' and key != 'samples' and key != 'patch_stats':
                all_metric_names.add(key)

        # Add test_stats sub-metrics
        test_stats = metrics.get('test_stats', {})
        for test_type, results in test_stats.items():
            if isinstance(results, dict):
                for result_type, count in results.items():
                    metric_name = f"test_stats.{test_type}.{result_type}"
                    all_metric_names.add(metric_name)

        # Add patch_stats sub-metrics
        patch_stats = metrics.get('patch_stats', {})
        for stat_name, stat_value in patch_stats.items():
            metric_name = f"patch_stats.{stat_name}"
            all_metric_names.add(metric_name)

    # Now collect values for each metric
    for metric_name in all_metric_names:
        metric_values[metric_name] = {}

        for model_name, metrics in models_data.items():
            value = None

            if '.' in metric_name:
                # Nested metric (e.g., test_stats.fail_to_pass.success)
                parts = metric_name.split('.')
                current = metrics
                for part in parts:
                    if isinstance(current, dict) and part in current:
                        current = current[part]
                    else:
                        current = None
                        break
                value = current
            else:
                # Top-level metric
                value = metrics.get(metric_name)

            # Only add if value is not None
            if value is not None:
                metric_values[metric_name][model_name] = value

    return metric_values


def create_chart_for_metric(
    metric_name: str,
    metric_data: Dict[str, Any],  # model_name -> value
    color_palette: List[Tuple[str, str]]
) -> Dict[str, Any]:
    """Create a Chart.js configuration for a single metric.

    Args:
        metric_name: Name of the metric (e.g., 'avg_acc')
        metric_data: Dictionary mapping model_name -> metric value
        color_palette: List of (border_color, background_color) pairs

    Returns:
        Chart.js configuration dictionary
    """
    # Sort models for consistent ordering
    model_names = sorted(metric_data.keys())
    values = [metric_data[model] for model in model_names]

    # Create chart
    chart = ChartJS(ChartType.BAR)

    # Set labels (model names)
    chart.set_labels(model_names)

    # Create dataset for this metric
    dataset = chart.create_dataset(ChartType.BAR)
    dataset.label = metric_name
    dataset.data = values

    # Assign colors to each bar based on model index
    background_colors = []
    border_colors = []

    for i, model_name in enumerate(model_names):
        color_idx = i % len(color_palette)
        border_color, background_color = color_palette[color_idx]
        background_colors.append(background_color)
        border_colors.append(border_color)

    dataset.backgroundColor = background_colors
    dataset.borderColor = border_colors
    dataset.borderWidth = 2
    dataset.hoverBackgroundColor = border_colors  # Use border color on hover
    dataset.hoverBorderColor = border_colors
    dataset.hoverBorderWidth = 3

    chart.add_dataset(dataset)

    # Configure chart title based on metric type
    title_text = f"SWE-bench: {metric_name}"

    # Add description for certain metric types
    descriptions = {
        'avg_acc': 'Average Accuracy',
        'acc_std': 'Accuracy Standard Deviation',
        'completed_rate': 'Completion Rate',
        'resolved_rate': 'Resolution Rate',
        'error_samples': 'Error Samples Count',
        'total_samples': 'Total Samples Count',
    }

    if metric_name in descriptions:
        title_text = f"SWE-bench: {descriptions[metric_name]}"

    # Configure chart options
    chart.set_title(
        title_text,
        font=Font(size=18, weight='bold')
    )

    chart.set_legend(
        display=True,
        position=Position.TOP,
        labels={"font": Font(size=12).to_dict()}
    )

    # Configure scales
    # Determine appropriate Y-axis range based on metric type
    y_min = 0
    y_max = None

    if metric_name in ['avg_acc', 'acc_std', 'completed_rate', 'resolved_rate']:
        # Rates and scores: typically 0-1 range
        y_max = 1.0
    elif metric_name == 'error_samples':
        # Error samples: 0 to max value + some padding
        max_val = max(values) if values else 0
        y_max = max_val * 1.1  # 10% padding
        if y_max == 0:
            y_max = 1.0  # Default range when all values are zero
    elif metric_name == 'total_samples':
        # Total samples: fixed at 50 (from data)
        y_max = 50
    elif 'test_stats' in metric_name:
        # Test stats counts: 0 to max value + padding
        max_val = max(values) if values else 0
        y_max = max_val * 1.1
        if y_max == 0:
            y_max = 1.0  # Default range when all values are zero

    y_scale = ScaleConfig(
        type=ScaleType.LINEAR,
        display=True,
        title=ScaleTitle(
            display=True,
            text='Value',
            color='#666',
            font=Font(size=14)
        ),
        min=y_min,
        max=y_max,
        grid=GridLine(
            display=True,
        )
    )

    x_scale = ScaleConfig(
        type=ScaleType.CATEGORY,
        display=True,
        title=ScaleTitle(
            display=True,
            text='Model',
            color='#666',
            font=Font(size=14)
        ),
        grid=GridLine(
            display=False
        )
    )

    chart.set_scales(x_scale=x_scale, y_scale=y_scale)

    # Add ticks callback for y-axis
    if 'y' in chart.scales:
        chart.scales['y']['ticks'] = {
            "display": True
        }

    chart.set_responsive(True, True)
    chart.set_animation(duration=1000, easing=Easing.EASE_OUT_QUART)

    # Get the chart configuration
    chart_config = chart.to_dict()

    # Ensure options exist
    chart_config.setdefault('options', {})

    # Set aspect ratio to control height (width:height = 2:1)
    chart_config['options']['aspectRatio'] = 2.0
    chart_config['options']['maintainAspectRatio'] = False

    # Ensure plugins section exists in options
    chart_config['options'].setdefault('plugins', {})

    # Add data labels plugin configuration to show values on bars
    # Use appropriate precision based on metric type
    precision = 4
    if metric_name in ['total_samples', 'error_samples'] or 'test_stats' in metric_name:
        precision = 0  # Integer values
    elif metric_name in ['avg_acc', 'acc_std', 'completed_rate', 'resolved_rate']:
        precision = 4  # Decimal values

    chart_config['options']['plugins']['datalabels'] = {
        "display": True,
        "font": {
            "size": 11,
            "weight": "bold"
        },
        "anchor": "end",
        "align": "top",
        "formatter": f"function(value) {{ return value !== null ? value.toFixed({precision}) : 'N/A'; }}",
        "backgroundColor": "rgba(255, 255, 255, 0.7)",
        "borderRadius": 3,
        "padding": 4
    }

    return chart_config


def main():
    parser = argparse.ArgumentParser(
        description='Generate Chart.js configurations for SWE-bench detailed metrics'
    )
    parser.add_argument(
        '--input',
        default='swebench_detailed_metrics.json',
        help='Input JSON file with detailed metrics (default: swebench_detailed_metrics.json)'
    )
    parser.add_argument(
        '--output',
        default='swebench_metrics_charts.json',
        help='Output JSON file path (default: swebench_metrics_charts.json)'
    )
    parser.add_argument(
        '--pretty',
        action='store_true',
        help='Generate pretty-printed JSON output'
    )
    parser.add_argument(
        '--include-samples',
        action='store_true',
        help='Include individual samples in metrics extraction (if available)'
    )

    args = parser.parse_args()

    # Check if input file exists
    if not os.path.exists(args.input):
        print(f"Error: Input file '{args.input}' not found!")
        print("Please run extract_swebench_metrics.py first to generate the metrics file.")
        return

    # Load metrics
    print(f"Loading metrics from {args.input}...")
    models_data = load_metrics(args.input)

    # Remove 'samples' from each model's data to reduce clutter (unless requested)
    if not args.include_samples:
        for model_name in models_data:
            models_data[model_name].pop('samples', None)

    print(f"Found {len(models_data)} models:")
    for model_name in models_data.keys():
        print(f"  - {model_name}")

    # Extract all metrics
    print("\nExtracting metrics...")
    all_metrics = extract_all_metrics(models_data)

    print(f"Found {len(all_metrics)} metrics:")
    for metric_name in sorted(all_metrics.keys()):
        print(f"  - {metric_name}")

    # Get color palette
    color_palette = get_color_palette()

    # Create chart configurations for each metric
    print("\nCreating chart configurations...")
    charts = {}

    for metric_name, metric_data in all_metrics.items():
        print(f"  Processing {metric_name}...")
        chart_config = create_chart_for_metric(metric_name, metric_data, color_palette)
        charts[metric_name] = chart_config

    # Write output file
    output_dir = os.path.dirname(args.output)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    indent = 2 if args.pretty else None
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(charts, f, indent=indent, ensure_ascii=False)

    print(f"\nSuccessfully created charts file: {args.output}")
    print(f"  - Total charts: {len(charts)}")
    print(f"  - Metrics included: {', '.join(sorted(charts.keys()))}")

    # Print sample of the output structure
    print("\nSample of output structure:")
    sample_keys = list(charts.keys())[:3] if len(charts) >= 3 else list(charts.keys())
    sample_data = {}
    for key in sample_keys:
        chart = charts[key]
        sample_data[key] = {
            "type": chart["type"],
            "data_labels": chart["data"]["labels"][:3] + ["..."] if chart["data"]["labels"] else [],
            "datasets_count": len(chart["data"]["datasets"])
        }

    print(json.dumps(sample_data, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()